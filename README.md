# APPLeNet: Visual Attention Parameterized Prompt Learning for Few-Shot Remote Sensing Image Generalization using CLIP

This repo contains the codebase of APPLeNet, which is one of the first works in Remote Sensing to perform unknown class and domain generalization using *prompt learning* by adapting pre-trained vision-language models (VLM) like [CLIP](https://arxiv.org/abs/2103.00020).

[![paper](https://img.shields.io/badge/arXiv-Paper-brightgreen)](https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/papers/Jha_APPLeNet_Visual_Attention_Parameterized_Prompt_Learning_for_Few-Shot_Remote_Sensing_CVPRW_2023_paper.pdf)
[![supplement](https://img.shields.io/badge/Supplementary-Material-F9D371)](https://openaccess.thecvf.com/content/CVPR2023W/EarthVision/supplemental/Jha_APPLeNet_Visual_Attention_CVPRW_2023_supplemental.pdf)
[![arXiv](https://img.shields.io/badge/arXiv-Paper-brightgreen)](https://arxiv.org/abs/2304.05995)
